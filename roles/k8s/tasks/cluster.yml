# code: language=Ansible insertSpaces=true tabSize=2


# Swap configuration
#
# By default, Kubernetes refuses to start when swapping is enabled.
# Disable swap partition and usage.

- name: Disable swap
  ansible.builtin.shell: |
    swapoff -a
  changed_when: true

- name: Unmount swap
  ansible.posix.mount:
    name: swap
    fstype: swap
    state: absent


# Primary master node selection
#
# A single master node must be selected to initialize a cluster.
# This master node is known as 'primary_master_name' in the subsequent tasks.

# First attempt
# Try to choose a primary master node among nodes which are of type 'master', already running and healthy.
- name: Select primary master node
  ansible.builtin.set_fact:
    primary_master_name: "{{ item }}"
    primary_master_ip: "{{ hostvars[item].ansible_default_ipv4.address }}"
    selection_msg: "cluster running; selected first active master node in cluster"
  delegate_to: localhost
  run_once: true
  when:
    - hostvars[item].jpclipffel_k8s_k8s_node_type == "master"
    - hostvars[item].jpclipffel_k8s_k8s_cluster_running
    - hostvars[item].jpclipffel_k8s_k8s_kubelet_healthy
    - primary_master_name is not defined or primary_master_name | length < 1
  with_items: "{{ ansible_play_hosts }}"

# Second attempt
# Try to choose a primary master node among nodes wich are of type 'master'
- name: Select primary master node
  ansible.builtin.set_fact:
    primary_master_name: "{{ item }}"
    primary_master_ip: "{{ hostvars[item].ansible_default_ipv4.address }}"
    selection_msg: "cluster not running; selected first master node in inventory"
  delegate_to: localhost
  run_once: true
  when:
    - hostvars[item].jpclipffel_k8s_k8s_node_type == "master"
    - primary_master_name is not defined or primary_master_name | length < 1
  with_items: "{{ ansible_play_hosts }}"

# Assert that a primary master node has been choosen
- name: Check if a primary node has been selected
  ansible.builtin.assert:
    that:
      - primary_master_name is defined and primary_master_name | length > 0
    fail_msg: "No primary master node selected"
    success_msg: "Selected primary master node '{{ primary_master_name }} @ {{ primary_master_ip }}: {{ selection_msg }}"


# Hosts file tampering
#
# Nodes's '/etc/hosts' files are temporarily tampered to resolve the control plane endpoint (primary master node) directly
# (i.e. ignoring potential VIP, LB, etc.). This is required during cluster bootstrap and rescale.
- name: Tamper nodes hosts file
  ansible.builtin.lineinfile:
    path: /etc/hosts
    regexp: '^{{ primary_master_ip }} {{ jpclipffel_k8s_k8s_control_plane_endpoint }}'
    line: "{{ primary_master_ip }} {{ jpclipffel_k8s_k8s_control_plane_endpoint }}"
    state: present
  when:
    - not jpclipffel_k8s_k8s_kubelet_healthy or (jpclipffel_k8s_k8s_node_type == 'master' and not jpclipffel_k8s_k8s_cluster_running) or 'force' in ansible_run_tags


# Cluster initialization
#
# The cluster is initialized on the previously selected 'primary_master_name' node,
# only if the cluster is not already running.
#
# The primary master hosts file is temporarily tampered to resolve the control plane
# endpoint to localhost (required when deploying an HA cluster).
- name: Cluster initialization
  block:

    - name: Cleanup local cluster state
      ansible.builtin.shell: >
        kubeadm reset -f
      changed_when: true

    # - name: Initialize cluster
    #   ansible.builtin.shell: >
    #     kubeadm init
    #     --apiserver-advertise-address "{{ jpclipffel_k8s_k8s_apiserver_advertise_address }}"
    #     --apiserver-bind-port "{{ jpclipffel_k8s_k8s_apiserver_bind_port }}"
    #     --upload-certs
    #     --control-plane-endpoint "{{ jpclipffel_k8s_k8s_control_plane_endpoint }}"
    #     --pod-network-cidr "{{ jpclipffel_k8s_k8s_pod_network_cidr }}"
    #     --service-cidr "{{ jpclipffel_k8s_k8s_service_cidr }}"
    #   changed_when: true

    - name: Initialize cluster
      ansible.builtin.shell: >
        kubeadm init
        --apiserver-advertise-address={{ jpclipffel_k8s_k8s_apiserver_advertise_address }}
        --apiserver-bind-port={{ jpclipffel_k8s_k8s_apiserver_bind_port }}
        --pod-network-cidr={{ jpclipffel_k8s_k8s_pod_network_cidr }}
        --service-cidr={{ jpclipffel_k8s_k8s_service_cidr }}
        --control-plane-endpoint={{ jpclipffel_k8s_k8s_control_plane_endpoint }}
        --upload-certs
      changed_when: true

    - name: Update kubeconfig
      ansible.builtin.copy:
        remote_src: true
        src: "{{ jpclipffel_k8s_k8s_kubeconfig_default }}"
        dest: "{{ jpclipffel_k8s_k8s_kubeconfig }}"
        mode: 0644

  when:
    - inventory_hostname == primary_master_name
    - not jpclipffel_k8s_k8s_cluster_running


# Access token generation
#
# A new access token is generated on the primary master node.
# This access token will be used to join the remaining master and worker nodes.
- block:

    - name: Generate temporary certificate key
      ansible.builtin.shell: |
        kubeadm {{ jpclipffel_k8s_k8s_version is version('1.20', '<') and 'alpha' or '' }} certs certificate-key
      register: jpclipffel_k8s_k8s_certificate_key

    - name: Upload cluster certificates
      ansible.builtin.shell: >
        kubeadm init phase upload-certs
        --upload-certs
        --certificate-key {{ jpclipffel_k8s_k8s_certificate_key.stdout }}

    - name: Generate cluster token
      ansible.builtin.shell: >
        kubeadm token create
        --ttl "{{ jpclipffel_k8s_k8s_token_ttl }}"
        --print-join-command
      register: jpclipffel_k8s_k8s_join_command

  when:
    - inventory_hostname == primary_master_name


# Master nodes joining
#
# Each remaining master node is joined using the previously generated token,
# only if it is not already a cluster member (by default).
# If the role is invoked with the tags 'force', the node is first reinitialized
# before being joined (== force join or re-join).
- block:

    - name: Join master node to cluster
      ansible.builtin.shell: |
        {% if 'force' in ansible_run_tags %}kubeadm reset -f && {% endif %}
        {{ hostvars[primary_master_name].jpclipffel_k8s_k8s_join_command.stdout  }} --control-plane --certificate-key {{ hostvars[primary_master_name].jpclipffel_k8s_k8s_certificate_key.stdout }}

    # - name: Replace KCM with Hyperkube
    #   ansible.builtin.replace:
    #     path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    #     regexp: '(?P<begin>^\s*image:\s+)(?P<url>[^:]+):(?P<tag>)(?P<rest>.*)'
    #     replace: '\g<begin>{{ jpclipffel_k8s_k8s_flavor_hyperkube_url }}:\g<tag>\g<rest>'
    #   when:
    #     - jpclipffel_k8s_k8s_flavor == "hyperkube"

    # - name: Replace Hyperkube with KCM
    #   ansible.builtin.replace:
    #     path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    #     regexp: '(?P<begin>^\s*image:\s+)(?P<url>[^:]+):(?P<tag>)(?P<rest>.*)'
    #     replace: '\g<begin>{{ jpclipffel_k8s_k8s_flavor_kcm_url }}:\g<tag>\g<rest>'
    #   when:
    #     - jpclipffel_k8s_k8s_flavor == "kcm"

    - name: Update kubeconfig
      ansible.builtin.copy:
        remote_src: true
        src: "{{ jpclipffel_k8s_k8s_kubeconfig_default }}"
        dest: "{{ jpclipffel_k8s_k8s_kubeconfig }}"
        mode: 0644

  when:
    - jpclipffel_k8s_k8s_node_type == "master"
    - inventory_hostname != primary_master_name
    - not jpclipffel_k8s_k8s_kubelet_healthy or not jpclipffel_k8s_k8s_cluster_running or 'force' in ansible_run_tags
    # - not jpclipffel_k8s_k8s_kubelet_healthy or 'force' in ansible_run_tags
    # - not jpclipffel_k8s_k8s_cluster_running or 'force' in ansible_run_tags


# Worker nodes joining
#
# Each worker node is joined using the previously generated token, only if it
# is not already a cluster member (by default).
# If the role is invoked with the tags 'force', the node is first reinitialized
# before being joined (== force join or re-join).

# - debug:
#     msg: "{{ hostvars[primary_master_name].jpclipffel_k8s_k8s_join_command.stdout }}"
#   when:
#     - jpclipffel_k8s_k8s_node_type == "worker"
#     - not jpclipffel_k8s_k8s_kubelet_healthy or 'force' in ansible_run_tags

- name: Join worker node to cluster
  ansible.builtin.shell: |
    {% if 'force' in ansible_run_tags %}kubeadm reset -f && {% endif %}
    {{ hostvars[primary_master_name].jpclipffel_k8s_k8s_join_command.stdout }}
  when:
    - jpclipffel_k8s_k8s_node_type == "worker"
    - not jpclipffel_k8s_k8s_kubelet_healthy or 'force' in ansible_run_tags


# Hosts file reset
#
# Nodes's '/etc/hosts' files were temporarily tampered to resolve the control plane endpoint (== primary master node) directly
# (i.e. ignoring potential VIP, LB, etc.). This was required only during cluster bootstrap and rescale.
- name: Cleanup tampered hosts file
  ansible.builtin.lineinfile:
    path: /etc/hosts
    regexp: '^{{ primary_master_ip }} {{ jpclipffel_k8s_k8s_control_plane_endpoint }}'
    state: absent
